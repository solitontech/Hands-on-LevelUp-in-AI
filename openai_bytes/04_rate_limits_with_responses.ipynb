{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65c9705",
   "metadata": {},
   "source": [
    "# Understanding and Handling OpenAI Rate Limits (with Responses API)\n",
    "\n",
    "This notebook walks through:\n",
    "\n",
    "- What **rate limits** are and why they exist  \n",
    "- How to **trigger a rate limit error** using `AsyncOpenAI` and multiple parallel requests  \n",
    "- How to **monitor token usage** using the **Responses API**  \n",
    "- Practical patterns for handling rate limits safely in production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0afdbd",
   "metadata": {},
   "source": [
    "## 1. What are rate limits?\n",
    "\n",
    "When you call the OpenAI API, you're sharing a pool of compute capacity with many other users.  \n",
    "To keep the system fair, fast, and stable, OpenAI enforces **rate limits** — caps on how much you can use the API over some window of time.\n",
    "\n",
    "Common types of limits include:\n",
    "\n",
    "- **Requests per minute (RPM)** – how many requests you can send per minute  \n",
    "- **Tokens per minute (TPM)** – how many input + output tokens you can use per minute  \n",
    "- Sometimes **requests per day / tokens per day**, depending on your plan\n",
    "\n",
    "If you exceed these limits, the API responds with **HTTP 429 (Rate limit)** and the SDK raises a `RateLimitError`.\n",
    "\n",
    "---\n",
    "\n",
    "### Why do rate limits exist?\n",
    "\n",
    "Rate limits help:\n",
    "\n",
    "- Prevent any single user from overloading the system  \n",
    "- Keep **latency predictable** for everyone  \n",
    "- Protect against accidental infinite loops or runaway scripts  \n",
    "- Let OpenAI plan capacity and pricing fairly\n",
    "\n",
    "---\n",
    "\n",
    "### How are rate limits enforced?\n",
    "\n",
    "At a high level, OpenAI tracks your usage across a rolling time window (for example: per minute).  \n",
    "If a new request would push you beyond your allowed RPM/TPM, that request is rejected with a rate‑limit error.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "- Your exact limits depend on your **account, plan, and model**.  \n",
    "- Limits can differ per **project** or **organization**.  \n",
    "- You can see your limits and usage in the [OpenAI dashboard](https://platform.openai.com/dashboard).\n",
    "\n",
    "We'll now move from theory to code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ee62f",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "Run the cell below **once** in your environment to install dependencies if you haven't already.\n",
    "\n",
    "- `openai` – official Python SDK\n",
    "- `python-dotenv` – optional, for loading `OPENAI_API_KEY` from a `.env` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2377104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies (uncomment if needed)\n",
    "# !pip install --upgrade openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938951e3",
   "metadata": {},
   "source": [
    "### Configure the OpenAI client\n",
    "\n",
    "You should set your API key in an environment variable:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```\n",
    "\n",
    "Or put it in a `.env` file alongside this notebook:\n",
    "\n",
    "```text\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "The code below:\n",
    "\n",
    "- Loads environment variables from `.env` if present\n",
    "- Creates both synchronous `OpenAI` and asynchronous `AsyncOpenAI` clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI, AsyncOpenAI, RateLimitError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env if present\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is not set. Please set it in your environment or .env file.\")\n",
    "\n",
    "# Synchronous and asynchronous clients\n",
    "client = OpenAI(api_key=api_key)\n",
    "async_client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "# We'll use a small, inexpensive model so it's cheap to experiment.\n",
    "DEFAULT_MODEL = \"gpt-5-nano\"  # adjust if this model name changes in your account\n",
    "model_list = client.models.list()\n",
    "print(\"Available models:\", [model.id for model in model_list.data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581f71e",
   "metadata": {},
   "source": [
    "## 3. Basic Responses API Call + Token Usage\n",
    "\n",
    "The **Responses API** is the recommended way to generate text.  \n",
    "Every response includes a `usage` object that reports how many tokens were consumed.\n",
    "\n",
    "Let's:\n",
    "1. Make a simple call.\n",
    "2. Inspect `response.usage`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Responses API call and usage inspection\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    input=\"\"\"Explain what API rate limits are in 2 short bullet points.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Model output:\\n\", response.output_text)\n",
    "\n",
    "print(\"\\nToken usage:\")\n",
    "print(response.usage)\n",
    "print(\"Input tokens:\", response.usage.input_tokens)\n",
    "print(\"Output tokens:\", response.usage.output_tokens)\n",
    "print(\"Total tokens:\", response.usage.total_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910ae6e",
   "metadata": {},
   "source": [
    "## 4. Tracking Token Usage Across Multiple Calls\n",
    "\n",
    "It's often useful to track cumulative token usage in your application for:\n",
    "\n",
    "- **Cost alerting / budgeting**\n",
    "- Staying under **tokens-per-minute (TPM)** limits\n",
    "- Logging / analytics\n",
    "\n",
    "Below is a helper wrapper that:\n",
    "\n",
    "- Calls the model\n",
    "- Logs the usage of each call\n",
    "- Keeps running totals for input, output, and total tokens\n",
    "\n",
    "You can also make use of external observability tools/libraries like [arize-phoenix](https://arize.com/docs/phoenix/get-started/get-started-tracing) to gain valuable insights based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20928ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenStats:\n",
    "    total_input_tokens: int = 0\n",
    "    total_output_tokens: int = 0\n",
    "    total_tokens: int = 0\n",
    "    history: List[dict] = field(default_factory=list)\n",
    "\n",
    "    def update(self, usage):\n",
    "        self.total_input_tokens += usage.input_tokens\n",
    "        self.total_output_tokens += usage.output_tokens\n",
    "        self.total_tokens += usage.total_tokens\n",
    "        self.history.append(\n",
    "            {\n",
    "                \"input_tokens\": usage.input_tokens,\n",
    "                \"output_tokens\": usage.output_tokens,\n",
    "                \"total_tokens\": usage.total_tokens,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def summary(self):\n",
    "        return {\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"num_calls\": len(self.history),\n",
    "        }\n",
    "\n",
    "\n",
    "token_stats = TokenStats()\n",
    "\n",
    "\n",
    "def ask_with_tracking(prompt: str, model: str = DEFAULT_MODEL) -> str:\n",
    "    \"\"\"Call the Responses API and update token stats.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt,\n",
    "    )\n",
    "\n",
    "    token_stats.update(response.usage)\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Response:\\n\", response.output_text)\n",
    "    print(\"Usage for this call:\", response.usage)\n",
    "    print(\"\\nRunning totals:\", token_stats.summary())\n",
    "    print(\"-\" * 60)\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "# Example: make a few calls and watch the cumulative usage grow\n",
    "ask_with_tracking(\"Give me 3 synonyms for 'fast'.\")\n",
    "ask_with_tracking(\"Give me 3 synonyms for 'slow'.\")\n",
    "ask_with_tracking(\"Explain what tokens are in one short paragraph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7fb1d",
   "metadata": {},
   "source": [
    "### Inspecting Rate-Limit Headers\n",
    "\n",
    "Some endpoints return rate-limit information in response headers.  \n",
    "You can inspect these headers using the `with_raw_response` helper.\n",
    "\n",
    "This can be useful to see:\n",
    "\n",
    "- How many requests/tokens you just used\n",
    "- How many you have left in the current window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30215db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect raw HTTP response headers to see rate-limit information (if present)\n",
    "\n",
    "raw = client.responses.with_raw_response.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    input=\"\"\"Say 'hello' in 3 different languages.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Status code:\", raw.status_code)\n",
    "\n",
    "print(\"\\nRate-limit related headers (if present):\")\n",
    "for name, value in raw.headers.items():\n",
    "    if \"ratelimit\" in name.lower():\n",
    "        print(f\"{name}: {value}\")\n",
    "\n",
    "parsed = raw.parse()\n",
    "print(\"\\nOutput:\", parsed.output_text)\n",
    "print(\"Token usage:\", parsed.usage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce441a",
   "metadata": {},
   "source": [
    "## 5. Handling Rate Limits Safely in Production\n",
    "\n",
    "In real applications, you should **never** just hammer the API until it breaks.  \n",
    "Instead, implement a **retry with exponential backoff** pattern when you see `RateLimitError` (429).\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "- Catch `openai.RateLimitError` specifically  \n",
    "- Wait (sleep) for a bit before retrying  \n",
    "- Increase the wait time with each retry (exponential backoff)  \n",
    "- Add **jitter** (a bit of randomness) so multiple workers don't synchronize their retries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01149b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import openai\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "async def safe_call_with_backoff(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    max_attempts: int = 5,\n",
    "    base_delay: float = 1.0,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Call the Responses API with retries and exponential backoff on rate limits.\"\"\"\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            response = await async_client.responses.create(\n",
    "                model=model,\n",
    "                input=prompt,\n",
    "                max_output_tokens=64,\n",
    "            )\n",
    "            print(\"Success on attempt\", attempt + 1)\n",
    "            print(\"Output:\", response.output_text)\n",
    "            print(\"Token usage:\", response.usage)\n",
    "            return response.output_text\n",
    "\n",
    "        except openai.RateLimitError as e:\n",
    "            attempt += 1\n",
    "            if attempt >= max_attempts:\n",
    "                print(\"Gave up after\", max_attempts, \"attempts due to rate limits.\")\n",
    "                raise\n",
    "\n",
    "            # Exponential backoff with jitter\n",
    "            delay = base_delay * (2 ** (attempt - 1))\n",
    "            delay = delay * (0.5 + random.random())  # add jitter between 0.5x and 1.5x\n",
    "            print(f\"Rate limit hit (attempt {attempt}). Sleeping for {delay:.2f} seconds...\")\n",
    "            await asyncio.sleep(delay)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Unexpected error:\", e)\n",
    "            raise\n",
    "\n",
    "\n",
    "# Example: use the safe caller\n",
    "async def demo_safe_call():\n",
    "    await safe_call_with_backoff(\"Give me a funny fact about turtles.\")\n",
    "\n",
    "\n",
    "await demo_safe_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e8433",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "1. Learned what **rate limits** are and why they exist.  \n",
    "2. Saw how to **inspect token usage** via `response.usage` and track cumulative tokens.  \n",
    "3. Used `with_raw_response` to inspect **rate-limit headers** (when present).  \n",
    "4. Implemented a **robust retry with exponential backoff** pattern using `RateLimitError`.\n",
    "\n",
    "You can now:\n",
    "\n",
    "- Adapt the tracking logic to feed dashboards, alerting, or cost controls  \n",
    "- Wrap all your API calls with a reusable `safe_call_with_backoff` helper  \n",
    "- Tune concurrency (`NUM_REQUESTS`) and delays to stay within your real limits\n",
    "\n",
    "Feel free to customize the code for your application's structure and logging style.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-level-up-in-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
