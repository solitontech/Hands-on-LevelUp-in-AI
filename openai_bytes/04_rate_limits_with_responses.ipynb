{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59be2ad1",
   "metadata": {},
   "source": [
    "# Rate Limits & Efficient Prompting — Using OpenAI Responses API\n",
    "\n",
    "\n",
    "This notebook explains API rate limits, practical strategies to handle them, and demonstrates how to use the OpenAI **Responses API** with robust retry/backoff logic, batching, and efficient prompting techniques. Run the code cells after setting your `OPENAI_API_KEY` in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d36b5",
   "metadata": {},
   "source": [
    "## Quick summary: What are Rate Limits?\n",
    "\n",
    "- **Rate limits** control how many requests or tokens you can send to the API in a time window.\n",
    "- You may encounter `429` HTTP errors when you exceed a limit.\n",
    "- Limits exist per-account, per-model, and per-endpoint. They can change; always check your account dashboard.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "1. Detecting and handling `429` errors with exponential backoff.\n",
    "2. Reducing request volume: batching, caching, and streaming.\n",
    "3. Reducing token usage: concise prompts, system messages, and few-shot design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc78ee",
   "metadata": {},
   "source": [
    "### Learn\n",
    "\n",
    "- **Why they exist:** protect service stability, ensure fair usage, and control costs.\n",
    "- **Common headers to read:** `x-ratelimit-limit`, `x-ratelimit-remaining`, `x-ratelimit-reset` (names may vary by provider). These headers tell you how many requests remain and when the window resets.\n",
    "- **Simple handling:** when you get a 429, pause (`sleep`) for a short time (exponential backoff) and retry.\n",
    "\n",
    "### Try\n",
    "\n",
    "Below are runnable examples:\n",
    "- a) a simulated quick-loop that shows hitting a rate limit and fixing with a sleep\n",
    "- b) a robust wrapper that inspects exceptions for rate-limit headers and waits accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "class SimulatedAPI:\n",
    "    def __init__(self, allowed_per_minute=10):\n",
    "        self.allowed = allowed_per_minute\n",
    "        self.calls = 0\n",
    "        self.window_start = time.time()\n",
    "\n",
    "    def call(self):\n",
    "        # reset window after 60s\n",
    "        now = time.time()\n",
    "        if now - self.window_start >= 60:\n",
    "            self.window_start = now\n",
    "            self.calls = 0\n",
    "        self.calls += 1\n",
    "        if self.calls > self.allowed:\n",
    "            # simulate 429 with headers\n",
    "            raise Exception('429 Too Many Requests', {'x-ratelimit-remaining': '0', 'x-ratelimit-reset': str(int(self.window_start + 60))})\n",
    "        return {'status': 200, 'data': f'result_{self.calls}'}\n",
    "\n",
    "sim = SimulatedAPI(allowed_per_minute=5)\n",
    "\n",
    "# Try calling quickly in a loop without pause (will trigger simulated 429)\n",
    "results = []\n",
    "for i in range(8):\n",
    "    try:\n",
    "        r = sim.call()\n",
    "        results.append(r)\n",
    "        print('OK', r)\n",
    "    except Exception as e:\n",
    "        err_msg, headers = e.args if len(e.args) > 1 else (str(e), {})\n",
    "        print('Error:', err_msg, 'headers=', headers)\n",
    "        print('Adding a short sleep to respect rate limits...')\n",
    "        time.sleep(5)  # naive fix\n",
    "        try:\n",
    "            r = sim.call()\n",
    "            results.append(r)\n",
    "            print('Retry OK', r)\n",
    "        except Exception as e2:\n",
    "            print('Retry still failed:', e2)\n",
    "print('Simulation done. Results collected:', len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this pattern around your openai.responses.create calls. It checks exception messages and optional headers\n",
    "# to determine how long to wait before retrying.\n",
    "import os\n",
    "try:\n",
    "    import openai\n",
    "except Exception:\n",
    "    raise ImportError(\"Please install the openai package: uv add openai\")\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY') or '<PUT_YOUR_API_KEY_HERE>'\n",
    "\n",
    "def call_with_rate_handling(fn, max_retries=6, base_delay=1.0, max_delay=60.0):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            # Try to extract HTTP status or headers if present\n",
    "            err_str = str(e).lower()\n",
    "            headers = {}\n",
    "            # Many HTTP client exceptions store headers in e.args or e.http_headers; we try common patterns.\n",
    "            if hasattr(e, 'headers') and isinstance(e.headers, dict):\n",
    "                headers = e.headers\n",
    "            elif len(e.args) > 1 and isinstance(e.args[1], dict):\n",
    "                headers = e.args[1]\n",
    "            remaining = headers.get('x-ratelimit-remaining') or headers.get('x-ratelimit_remaining') or None\n",
    "            reset = headers.get('x-ratelimit-reset') or headers.get('x-ratelimit_reset') or None\n",
    "            is_429 = '429' in err_str or 'rate limit' in err_str or 'too many' in err_str\n",
    "            if not is_429 or attempt > max_retries:\n",
    "                print('Not retrying; re-raising. Error:', e)\n",
    "                raise\n",
    "            # If the server told us when it resets, use that\n",
    "            if reset:\n",
    "                try:\n",
    "                    reset_ts = int(reset)\n",
    "                    wait = max(0, reset_ts - int(time.time()))\n",
    "                    wait = min(wait, max_delay)\n",
    "                    print(f'Received reset header; sleeping {wait}s before retry (headers: {headers})')\n",
    "                except Exception:\n",
    "                    wait = min(max_delay, base_delay * (2 ** (attempt - 1)))\n",
    "                    wait = wait * random.uniform(0.5, 1.0)\n",
    "                    print(f'Could not parse reset header; using backoff {wait:.1f}s (attempt {attempt})')\n",
    "            else:\n",
    "                wait = min(max_delay, base_delay * (2 ** (attempt - 1)))\n",
    "                wait = wait * random.uniform(0.5, 1.0)\n",
    "                print(f'Rate limited; backing off {wait:.1f}s (attempt {attempt})')\n",
    "            time.sleep(wait)\n",
    "\n",
    "# Example usage with OpenAI Responses API (commented out — enable when you have API access):\n",
    "def make_api_call():\n",
    "    return openai.responses.create(model='gpt-4.1-mini', input=[{'role':'user','content':'Explain rate limits in one sentence.'}])\n",
    "resp = call_with_rate_handling(make_api_call)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643864cd",
   "metadata": {},
   "source": [
    "## Example: Basic Responses API call\n",
    "\n",
    "A simple example using the `openai` Python client and `responses` API. Replace the model with one available to your account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY') or '<PUT_YOUR_API_KEY_HERE>'\n",
    "\n",
    "def simple_response_example():\n",
    "    print(\"Sending a simple prompt to Responses API (example).\")\n",
    "    resp = openai.responses.create(\n",
    "        model=\"gpt-4.1-mini\",  # replace with a model you have access to\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise assistant that answers in plain English.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Explain rate limits in two sentences.\"}\n",
    "        ]\n",
    "    )\n",
    "    # The response shape may contain generative content in resp.output\n",
    "    print(\"Response object keys:\", list(resp.model_dump().keys()))\n",
    "    # Try to print text output (SDK may return different structure depending on version)\n",
    "    try:\n",
    "        # Many SDK versions: resp.output[0].content[0].text\n",
    "        outputs = resp.output\n",
    "        if isinstance(outputs, list):\n",
    "            # Flatten simple case\n",
    "            texts = []\n",
    "            for item in outputs:\n",
    "                if isinstance(item, dict):\n",
    "                    for c in item.get('content', []):\n",
    "                        if isinstance(c, dict) and 'text' in c:\n",
    "                            texts.append(c['text'])\n",
    "                        elif isinstance(c, str):\n",
    "                            texts.append(c)\n",
    "                elif isinstance(item, str):\n",
    "                    texts.append(item)\n",
    "            print('\\n'.join(texts))\n",
    "        else:\n",
    "            print(outputs)\n",
    "    except Exception as e:\n",
    "        print('Could not parse text output from response:', e)\n",
    "\n",
    "# Run example (comment out if you don't want to call the API now)\n",
    "simple_response_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b572fec",
   "metadata": {},
   "source": [
    "## Robust retry/backoff helper (handles 429s and transient errors)\n",
    "\n",
    "This helper retries on HTTP 429 (rate limit) and on some transient server errors. It uses exponential backoff with jitter.\n",
    "Use this wrapper around any API call that may be rate-limited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43917887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "def retry_with_backoff(fn, max_retries=6, base_delay=1.0, max_delay=60.0, allowed_exceptions=(Exception,)):\n",
    "    \"\"\"Call fn() and retry on exceptions with exponential backoff + jitter.\n",
    "    fn should be a callable that performs an API call and either returns a value or raises an Exception.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            # If it's an OpenAI HTTP error with status 429 or a requests rate-limit, retry\n",
    "            err_str = str(e).lower()\n",
    "            is_rate = ('429' in err_str) or ('rate limit' in err_str) or ('rate_limit' in err_str)\n",
    "            is_transient = isinstance(e, RequestException) or 'timed out' in err_str or 'tempor' in err_str\n",
    "            if attempt > max_retries or not (is_rate or is_transient):\n",
    "                print('Not retrying; re-raising exception. Last error:', e)\n",
    "                raise\n",
    "            # exponential backoff with full jitter\n",
    "            sleep = min(max_delay, base_delay * (2 ** (attempt - 1)))\n",
    "            sleep = sleep * random.uniform(0.5, 1.0)\n",
    "            print(f\"Retry {attempt}/{max_retries} after error: {e}. Sleeping {sleep:.2f}s...\")\n",
    "            time.sleep(sleep)\n",
    "\n",
    "# Example usage wrapping the Responses API call\n",
    "def responses_with_retry(prompt_text):\n",
    "    def call():\n",
    "        return openai.responses.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            input=[{\"role\":\"system\",\"content\":\"You are helpful.\"},\n",
    "                   {\"role\":\"user\",\"content\": prompt_text}],\n",
    "            max_output_tokens=300\n",
    "        )\n",
    "    return retry_with_backoff(call)\n",
    "\n",
    "# Example (commented out):\n",
    "r = responses_with_retry('Explain exponential backoff and why it helps with rate limits.') \n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbea58e",
   "metadata": {},
   "source": [
    "## Efficient prompting patterns\n",
    "\n",
    "Tips to reduce tokens and improve output quality:\n",
    "\n",
    "1. **System message**: Put stable instructions in the system role so you don't repeat them in every request.\n",
    "2. **Be concise**: Use short, direct prompts. Avoid long unnecessary context.\n",
    "3. **Few-shot only when necessary**: Provide 1–3 examples, not dozens.\n",
    "4. **Use `max_output_tokens`** to cap responses and avoid runaway outputs.\n",
    "5. **Use structured output** (JSON) or `function_call`/tools when you need machine-parseable data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10081efb",
   "metadata": {},
   "source": [
    "### Example: concise vs verbose prompt\n",
    "\n",
    "Compare two prompts and see token differences. We'll show how to call the Responses API with a short and a long prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_prompt = \"Give me a 2-line summary of why API rate limits exist.\"\n",
    "long_prompt = \"\"\"You are an expert technical writer. Please write a detailed multi-paragraph explanation of why API rate limits exist, \"\n",
    "including historical context, server architecture reasons, and economic considerations. Include many examples and suggestions for developers.\"\n",
    "\"\"\"\n",
    "\n",
    "print('Short prompt length (chars):', len(short_prompt))\n",
    "print('Long prompt length (chars):', len(long_prompt))\n",
    "\n",
    "# You can measure approximate token usage via tiktoken (if installed) or estimate via simple heuristics.\n",
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('gpt-4o-mini') if hasattr(tiktoken, 'encoding_for_model') else tiktoken.get_encoding('cl100k_base')\n",
    "    s_tokens = len(enc.encode(short_prompt))\n",
    "    l_tokens = len(enc.encode(long_prompt))\n",
    "    print('Estimated short prompt tokens:', s_tokens)\n",
    "    print('Estimated long prompt tokens:', l_tokens)\n",
    "except Exception:\n",
    "    print('tiktoken not available; install tiktoken to get token-level estimates (pip install tiktoken).')\n",
    "\n",
    "# Example calls (commented out to avoid accidental API calls)\n",
    "# r_short = responses_with_retry(short_prompt)\n",
    "# r_long = responses_with_retry(long_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab909f1",
   "metadata": {},
   "source": [
    "## Batching & Stream strategies\n",
    "\n",
    "- **Batching**: Combine multiple small requests into a single request when possible (e.g., send a list of short inputs in one call). This reduces per-request overhead and rate-limit pressure.\n",
    "- **Streaming**: If supported, streaming reduces perceived latency and can be useful for long outputs; it doesn't necessarily reduce tokens but improves responsiveness.\n",
    "\n",
    "Example: how to batch multiple independent prompts in one Responses API call by joining inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple batching pattern: join inputs and include separators; parse outputs afterward.\n",
    "prompts = [\n",
    "    'Summarize the concept of rate limiting in one line.',\n",
    "    'Give one tip to reduce API usage for developers.'\n",
    "]\n",
    "\n",
    "batched_input = '\\n---\\n'.join(prompts)\n",
    "print('Batched input length:', len(batched_input))\n",
    "\n",
    "# Example call (commented out):\n",
    "resp = responses_with_retry(batched_input)\n",
    "print(resp)\n",
    "\n",
    "# Note: alternatively you can send a list of messages in the input array and post-process the response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92acd3",
   "metadata": {},
   "source": [
    "## Saving Cost and Time\n",
    "\n",
    "### Learn\n",
    "\n",
    "- **Prompt caching:** store input → output mappings so repeated identical prompts reuse cached results instead of calling the API again.\n",
    "- **Smaller prompts:** remove unnecessary context; summarize long histories before sending.\n",
    "- **Pick smaller models:** use cheaper models for simple tasks and route hard tasks to larger models (task routing).\n",
    "- **Stop early:** use `max_output_tokens` (or `max_tokens`) to cap output length and control cost.\n",
    "- **Memory reuse:** keep recent useful outputs in memory rather than re-requesting.\n",
    "\n",
    "### Try\n",
    "\n",
    "Below is an interactive comparison: ask the same question twice — once using caching, once without — and compare time and (simulated) cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd67ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Prompt caching example (REAL OpenAI Responses API)\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\") or \"<PUT_YOUR_KEY_HERE>\"\n",
    "\n",
    "CACHE_DIR = '.prompt_cache'\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def cache_key(prompt):\n",
    "    return hashlib.sha256(prompt.encode('utf-8')).hexdigest()\n",
    "\n",
    "def save_cache(prompt, response_text):\n",
    "    key = cache_key(prompt)\n",
    "    path = os.path.join(CACHE_DIR, key + '.json')\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'prompt': prompt, 'response': response_text, 'ts': time.time()}, f)\n",
    "    return path\n",
    "\n",
    "def load_cache(prompt):\n",
    "    key = cache_key(prompt)\n",
    "    path = os.path.join(CACHE_DIR, key + '.json')\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "# ---- REAL Responses API call ----\n",
    "def call_llm(prompt):\n",
    "    \"\"\"Call OpenAI Responses API and return plain text output.\"\"\"\n",
    "    resp = openai.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": \"Provide concise helpful answers.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_output_tokens=200\n",
    "    )\n",
    "\n",
    "    # Extract text safely from resp.output\n",
    "    txt = \"\"\n",
    "    try:\n",
    "        for item in resp.output:\n",
    "            for c in item.content:\n",
    "                if hasattr(c, \"text\"):\n",
    "                    txt += c.text\n",
    "    except Exception:\n",
    "        txt = str(resp)\n",
    "    return txt.strip()\n",
    "\n",
    "# ---- Compare cached vs uncached ----\n",
    "prompt = \"List three quick tips to reduce API costs.\"\n",
    "\n",
    "print(\"First call (no cache)\")\n",
    "t0 = time.time()\n",
    "cached = load_cache(prompt)\n",
    "if cached:\n",
    "    print(\"Loaded from cache:\", cached[\"response\"])\n",
    "else:\n",
    "    out = call_llm(prompt)\n",
    "    save_cache(prompt, out)\n",
    "    print(\"API response:\", out)\n",
    "t1 = time.time()\n",
    "print(\"First call took: {:.3f}s\".format(t1 - t0))\n",
    "\n",
    "print(\"\\nSecond call (with cache)\")\n",
    "t0 = time.time()\n",
    "cached2 = load_cache(prompt)\n",
    "if cached2:\n",
    "    print(\"Loaded from cache:\", cached2[\"response\"])\n",
    "else:\n",
    "    out2 = call_llm(prompt)\n",
    "    save_cache(prompt, out2)\n",
    "    print(\"API response:\", out2)\n",
    "t1 = time.time()\n",
    "print(\"Second call took: {:.5f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa105dd",
   "metadata": {},
   "source": [
    "## Monitoring Cost and Usage\n",
    "\n",
    "### Learn\n",
    "\n",
    "- Inspect `response.usage` to see `prompt_tokens`, `completion_tokens`, and `total_tokens` (SDKs may differ in naming).  \n",
    "- Log tokens, time, and estimated cost per call.  \n",
    "- Keep running totals per day to estimate spend.  \n",
    "- Cost formula: `(input_tokens × rate_in) + (output_tokens × rate_out)` — rates depend on model and should be retrieved from your billing/pricing page.  \n",
    "- Review your actual usage and billing on the OpenAI dashboard: https://platform.openai.com/usage\n",
    "\n",
    "### Try\n",
    "\n",
    "Implement a `track_cost()` helper below to log usage and running totals. Example uses simulated `usage` objects but demonstrates how to integrate real `response['usage']` values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Responses API call + robust extraction and logging (handles input_tokens/output_tokens)\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import openai\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\") or \"<PUT_YOUR_API_KEY_HERE>\"\n",
    "\n",
    "MODEL = \"gpt-4.1-mini\"       # replace with a model you have access to\n",
    "FILE_PATH = \"/mnt/data/03_function_calling_and_tools.ipynb\"  # the local path (runner will convert to URL)\n",
    "\n",
    "LOG_FILE = 'api_usage_log.csv'\n",
    "fieldnames = ['ts', 'prompt_tokens', 'completion_tokens', 'total_tokens', 'duration_s', 'estimated_cost_usd']\n",
    "\n",
    "\n",
    "def estimate_cost(usage, rate_in_per_token=0.000003, rate_out_per_token=0.000006): # update according to model pricing\n",
    "    \"\"\"Estimate cost in USD. Update rates to match your model pricing (per-token costs).\"\"\"\n",
    "    in_t = usage.get('prompt_tokens', 0)\n",
    "    out_t = usage.get('completion_tokens', 0)\n",
    "    return in_t * rate_in_per_token + out_t * rate_out_per_token\n",
    "\n",
    "def track_cost(usage, duration_s, rate_in_per_token=0.000003, rate_out_per_token=0.000006):\n",
    "    cost = estimate_cost(usage, rate_in_per_token, rate_out_per_token)\n",
    "    row = {\n",
    "        'ts': int(time.time()),\n",
    "        'prompt_tokens': usage.get('prompt_tokens', 0),\n",
    "        'completion_tokens': usage.get('completion_tokens', 0),\n",
    "        'total_tokens': usage.get('total_tokens', usage.get('prompt_tokens',0)+usage.get('completion_tokens',0)),\n",
    "        'duration_s': round(duration_s, 3),\n",
    "        'estimated_cost_usd': round(cost, 10)\n",
    "    }\n",
    "    write_header = not os.path.exists(LOG_FILE)\n",
    "    with open(LOG_FILE, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "    print('Logged:', row)\n",
    "    return row\n",
    "\n",
    "# --------- Robust usage extractor ---------\n",
    "def _int_from_maybe_object(val):\n",
    "    \"\"\"Extract integer count from a field which might be int, str, or an object with numeric attributes.\"\"\"\n",
    "    if val is None:\n",
    "        return 0\n",
    "    if isinstance(val, int):\n",
    "        return val\n",
    "    if isinstance(val, str) and val.isdigit():\n",
    "        return int(val)\n",
    "    # It may be an SDK object like InputTokensDetails(cached_tokens=0) or similar; try attributes\n",
    "    for attr in ('total', 'value', 'cached_tokens', 'count', 'n'):\n",
    "        if hasattr(val, attr):\n",
    "            try:\n",
    "                return int(getattr(val, attr))\n",
    "            except Exception:\n",
    "                pass\n",
    "    # try dict-like\n",
    "    try:\n",
    "        if isinstance(val, dict):\n",
    "            for k in ('total', 'value', 'cached_tokens', 'count', 'n'):\n",
    "                if k in val:\n",
    "                    return int(val[k])\n",
    "    except Exception:\n",
    "        pass\n",
    "    # As a last resort, try to coerce to int\n",
    "    try:\n",
    "        return int(val)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def extract_usage(resp):\n",
    "    \"\"\"\n",
    "    Return a normalized usage dict:\n",
    "      {'prompt_tokens': int, 'completion_tokens': int, 'total_tokens': int}\n",
    "    Handles multiple SDK variants: input_tokens/output_tokens, prompt_tokens/completion_tokens, or nested places.\n",
    "    \"\"\"\n",
    "    usage = {}\n",
    "    # Common top-level places\n",
    "    candidates = []\n",
    "    try:\n",
    "        if isinstance(resp, dict):\n",
    "            candidates.append(resp.get('usage'))\n",
    "            candidates.append(resp.get('response', {}).get('usage'))\n",
    "            candidates.append(resp.get('metadata', {}).get('usage') if isinstance(resp.get('metadata'), dict) else None)\n",
    "            # Also add resp keys directly\n",
    "            candidates.append({k: resp.get(k) for k in ('input_tokens','output_tokens','total_tokens','prompt_tokens','completion_tokens')})\n",
    "        # object-like resp\n",
    "        if hasattr(resp, 'usage'):\n",
    "            candidates.append(getattr(resp, 'usage'))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Flatten & find numbers\n",
    "    found = {}\n",
    "    for cand in candidates:\n",
    "        if not cand:\n",
    "            continue\n",
    "        # if it's dict-like\n",
    "        if isinstance(cand, dict):\n",
    "            for k,v in cand.items():\n",
    "                found[k] = v\n",
    "        else:\n",
    "            # try to inspect attributes (SDK objects)\n",
    "            for attr in ('input_tokens','output_tokens','total_tokens','prompt_tokens','completion_tokens'):\n",
    "                if hasattr(cand, attr):\n",
    "                    found[attr] = getattr(cand, attr)\n",
    "\n",
    "    # Map different key names to normalized fields\n",
    "    prompt_t = None\n",
    "    completion_t = None\n",
    "    total_t = None\n",
    "\n",
    "    # input_tokens / output_tokens (newer SDK) -> prompt_tokens/completion_tokens\n",
    "    if 'input_tokens' in found:\n",
    "        prompt_t = _int_from_maybe_object(found.get('input_tokens'))\n",
    "    if 'output_tokens' in found:\n",
    "        completion_t = _int_from_maybe_object(found.get('output_tokens'))\n",
    "\n",
    "    # fallback to explicit prompt_tokens/completion_tokens\n",
    "    if prompt_t is None and 'prompt_tokens' in found:\n",
    "        prompt_t = _int_from_maybe_object(found.get('prompt_tokens'))\n",
    "    if completion_t is None and 'completion_tokens' in found:\n",
    "        completion_t = _int_from_maybe_object(found.get('completion_tokens'))\n",
    "\n",
    "    # total tokens\n",
    "    if 'total_tokens' in found:\n",
    "        total_t = _int_from_maybe_object(found.get('total_tokens'))\n",
    "\n",
    "    # If total exists but prompt/completion do not, attempt to split proportionally (best-effort)\n",
    "    if total_t is not None and (prompt_t is None and completion_t is None):\n",
    "        prompt_t = 0\n",
    "        completion_t = total_t\n",
    "\n",
    "    # If prompt/completion exist but total doesn't, compute it\n",
    "    if total_t is None:\n",
    "        total_t = (prompt_t or 0) + (completion_t or 0)\n",
    "\n",
    "    # final defaults\n",
    "    prompt_t = int(prompt_t or 0)\n",
    "    completion_t = int(completion_t or 0)\n",
    "    total_t = int(total_t or 0)\n",
    "\n",
    "    return {\n",
    "        'prompt_tokens': prompt_t,\n",
    "        'completion_tokens': completion_t,\n",
    "        'total_tokens': total_t\n",
    "    }\n",
    "\n",
    "# --------- Extract text from resp.output ---------\n",
    "def extract_text_from_output(resp):\n",
    "    \"\"\"\n",
    "    Extract readable text from resp.output which may be a list of ResponseOutputMessage objects,\n",
    "    each with content list of ResponseOutputText objects that have .text attribute.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    try:\n",
    "        # dict-like\n",
    "        if isinstance(resp, dict) and 'output' in resp:\n",
    "            out = resp['output']\n",
    "        elif hasattr(resp, 'output'):\n",
    "            out = getattr(resp, 'output')\n",
    "        else:\n",
    "            out = None\n",
    "\n",
    "        if not out:\n",
    "            return \"\"\n",
    "\n",
    "        # iterate items\n",
    "        for item in out:\n",
    "            # item could be dict-like or object\n",
    "            content = None\n",
    "            if isinstance(item, dict):\n",
    "                content = item.get('content') or item.get('text') or item.get('message') or []\n",
    "            else:\n",
    "                content = getattr(item, 'content', None) or getattr(item, 'text', None) or []\n",
    "\n",
    "            # content is often a list of content pieces\n",
    "            if isinstance(content, list):\n",
    "                for piece in content:\n",
    "                    if isinstance(piece, dict) and 'text' in piece:\n",
    "                        texts.append(piece['text'])\n",
    "                    else:\n",
    "                        # object-like piece\n",
    "                        txt = getattr(piece, 'text', None)\n",
    "                        if txt:\n",
    "                            texts.append(txt)\n",
    "                        elif isinstance(piece, str):\n",
    "                            texts.append(piece)\n",
    "            elif isinstance(content, str):\n",
    "                texts.append(content)\n",
    "            else:\n",
    "                # try to string-convert the item\n",
    "                try:\n",
    "                    texts.append(str(item))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"\\\\n\".join(t for t in texts if t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------- Simple retry wrapper for transient errors / 429s ---------\n",
    "def call_with_retry(fn, max_retries=5, base_delay=1.0, max_delay=30.0):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            err_s = str(e).lower()\n",
    "            is_rate = ('429' in err_s) or ('rate limit' in err_s) or ('too many' in err_s)\n",
    "            is_transient = 'timed out' in err_s or 'timeout' in err_s or isinstance(e, TimeoutError)\n",
    "            if attempt > max_retries or not (is_rate or is_transient):\n",
    "                print(\"Not retrying; raising:\", e)\n",
    "                raise\n",
    "            sleep = min(max_delay, base_delay * (2 ** (attempt - 1)))\n",
    "            sleep = sleep * random.uniform(0.6, 1.0)\n",
    "            print(f\"Attempt {attempt}/{max_retries} failed with '{e}'. Sleeping {sleep:.2f}s before retry.\")\n",
    "            time.sleep(sleep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------- Build prompt and make call ---------\n",
    "prompt = (\n",
    "    f\"Summarize the notebook at this local path (runner will convert to a URL): {FILE_PATH}\\\\n\\\\n\"\n",
    "    \"Provide a concise 3-bullet summary. If file is inaccessible, say so.\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise assistant that summarizes notebooks in bullet points.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "def make_call_and_track():\n",
    "    start = time.time()\n",
    "    def call():\n",
    "        return openai.responses.create(\n",
    "            model=MODEL,\n",
    "            input=messages,\n",
    "            max_output_tokens=300\n",
    "        )\n",
    "    resp = call_with_retry(call)\n",
    "    duration = time.time() - start\n",
    "\n",
    "    # Extract text and print\n",
    "    text = extract_text_from_output(resp)\n",
    "    print(\"\\\\n--- Model Output (clean) ---\")\n",
    "    print(text if text else \"(no textual output extracted)\")\n",
    "\n",
    "    # Normalize usage and track cost\n",
    "    usage = extract_usage(resp)\n",
    "    print(\"\\\\n--- Normalized usage ---\")\n",
    "    print(usage)\n",
    "    print(\"-\"*30)\n",
    "\n",
    "    # Ensure total exists\n",
    "    if 'total_tokens' not in usage:\n",
    "        usage['total_tokens'] = usage.get('prompt_tokens', 0) + usage.get('completion_tokens', 0)\n",
    "\n",
    "    # Log with track_cost\n",
    "    try:\n",
    "        row = track_cost(usage, duration)\n",
    "        print(\"-\"*30)\n",
    "        print(\"\\\\nTracked cost:\", row)\n",
    "        print(\"-\"*30)\n",
    "    except Exception as e:\n",
    "        print(\"track_cost raised an exception:\", e)\n",
    "\n",
    "    return resp, usage, text\n",
    "\n",
    "# ---- Run it (uncomment to execute) ----\n",
    "resp, usage, text = make_call_and_track()\n",
    "print(\"Done. Extracted usage:\", usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1ea7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-level-up-in-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
