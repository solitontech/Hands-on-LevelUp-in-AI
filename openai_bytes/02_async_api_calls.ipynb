{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674b9400",
   "metadata": {},
   "source": [
    "# Byte #2: Calling Models Asynchronously (Fast Calls) ‚ö°\n",
    "\n",
    "**‚è±Ô∏è Time to Complete: 5-10 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd73add",
   "metadata": {},
   "source": [
    "## AsyncIO in Python\n",
    " \n",
    "Asynchronous programming lets Python juggle multiple tasks without waiting for each one to finish before starting the next. Instead of blocking on slow operations, code yields control back to an event loop, which resumes work as soon as results are ready.\n",
    " \n",
    "Picture a restaurant shift with one chef:\n",
    " \n",
    "- **üêå Synchronous shift:** Cook one order to completion before starting the next‚Äîlong waits when dishes take time.\n",
    "- **‚ö° AsyncIO shift:** Prep several orders, sending each to simmer while you season the next‚Äîthe chef keeps moving, and diners eat sooner.\n",
    " \n",
    "### Core Ideas\n",
    "- **Event loop:** The scheduler that keeps track of what should run next.\n",
    "- **Coroutines (`async def`):** Functions that can pause themselves with `await` and resume later.\n",
    "- **Awaitables:** Objects you can `await`‚Äîcoroutines, tasks, or anything implementing `__await__`.\n",
    "- **Tasks:** Wrappers created with `asyncio.create_task` so the event loop can run multiple coroutines concurrently.\n",
    "- **Non-blocking I/O:** Ideal use case‚Äînetwork calls, file reads, timers‚Äîanything that spends time waiting on external work.\n",
    " \n",
    "AsyncIO doesn‚Äôt give Python extra CPU cores; it simply keeps your program busy while I/O waits resolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38b97d",
   "metadata": {},
   "source": [
    "## Why Use Async with OpenAI?\n",
    " \n",
    "Now that you know how AsyncIO keeps Python busy while I/O waits resolve, apply the same pattern to OpenAI calls. Every request is network-bound, so overlapping them can slash total runtime when you have multiple prompts to evaluate.\n",
    " \n",
    "Use async when you need to:\n",
    "- Ask multiple models the same question (compare responses)\n",
    "- Process many questions at once\n",
    "- Call different APIs simultaneously\n",
    "- Save time when making multiple API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bb9d0",
   "metadata": {},
   "source": [
    "### AsyncIO Building Blocks in Practice\n",
    " \n",
    "1. Define coroutines with `async def`.\n",
    "2. Pause inside those coroutines with `await` whenever you hit an I/O wait.\n",
    "3. Group coroutines into tasks so the event loop can juggle them.\n",
    "4. Kick off the event loop with `await main()` in notebooks or `asyncio.run(main())` in scripts.\n",
    " \n",
    "This pattern scales from simple timers to complex services that stream data, call APIs, or orchestrate background jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b755d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def boil_pasta(name: str, seconds: int) -> str:\n",
    "    \"\"\"Simulate a slow kitchen task.\"\"\"\n",
    "    await asyncio.sleep(seconds)\n",
    "    return f\"{name} done in {seconds}s\"\n",
    "\n",
    "async def main():\n",
    "    # Start two pots at once; the event loop keeps track of both timers.\n",
    "    tasks = [asyncio.create_task(boil_pasta(\"Rigatoni\", 2)),\n",
    "             asyncio.create_task(boil_pasta(\"Farfalle\", 3))]\n",
    "\n",
    "    for result in await asyncio.gather(*tasks):\n",
    "        print(result)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47846250",
   "metadata": {},
   "source": [
    "## The Performance Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd97e24",
   "metadata": {},
   "source": [
    "### Side-by-Side Timing Sketch\n",
    "\n",
    "\n",
    "```text\n",
    "Synchronous (one at a time)\n",
    "- Call Model 1 ‚Üí wait 2 seconds\n",
    "- Call Model 2 ‚Üí wait 2 seconds\n",
    "- Call Model 3 ‚Üí wait 2 seconds\n",
    "- Total: 6 seconds\n",
    "\n",
    "Asynchronous (all at once)\n",
    "- Dispatch all 3 requests together\n",
    "- Wait 2 seconds while they run in parallel\n",
    "- Total: 2 seconds ‚ú®\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499bb24",
   "metadata": {},
   "source": [
    "## Setup: Install Async OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb622de1",
   "metadata": {},
   "source": [
    "## Basic Async Example: Single Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "async def ask_ai(question):\n",
    "    \"\"\"Ask a question asynchronously using the Responses API.\"\"\"\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    response = await client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": question}],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "async def main():\n",
    "    answer = await ask_ai(\"What is Python?\")\n",
    "    print(answer)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4be12e",
   "metadata": {},
   "source": [
    "**Key Differences from Sync:**\n",
    "1. Use `AsyncOpenAI` instead of `OpenAI`\n",
    "2. Functions are defined with `async def`\n",
    "3. Use `await` before async operations\n",
    "4. Run the coroutine with `await main()` here (or `asyncio.run(main())` in a standalone script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c21281",
   "metadata": {},
   "source": [
    "## Calling Multiple Models at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccab0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "import time\n",
    "\n",
    "async def ask_model(client, model, question):\n",
    "    \"\"\"Ask one model a question via the Responses API.\"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    response = await client.responses.create(\n",
    "        model=model,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": question}],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    duration = time.time() - start\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"answer\": response.output_text,\n",
    "        \"tokens\": response.usage.total_tokens,\n",
    "        \"time\": duration,\n",
    "    }\n",
    "\n",
    "async def compare_models(question):\n",
    "    \"\"\"Ask the same question to multiple models simultaneously.\"\"\"\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    # List of models to compare\n",
    "    models = [\"gpt-5-nano\", \"gpt-4.1\"]\n",
    "\n",
    "    # Create tasks for all models\n",
    "    tasks = [ask_model(client, model, question) for model in models]\n",
    "\n",
    "    # Run all tasks in parallel\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    return results\n",
    "\n",
    "async def main():\n",
    "    print(\"Asking multiple models the same question...\\n\")\n",
    "\n",
    "    results = await compare_models(\"Explain async programming in one sentence\")\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"Model: {result['model']}\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"Tokens: {result['tokens']}\")\n",
    "        print(f\"Time: {result['time']:.2f}s\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cd133",
   "metadata": {},
   "source": [
    "## Speed Comparison: Sync vs Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500494ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"What is JavaScript?\",\n",
    "]\n",
    "\n",
    "\n",
    "# SYNCHRONOUS VERSION (Slow)\n",
    "def sync_multiple_calls():\n",
    "    \"\"\"Make 5 calls one at a time using the Responses API.\"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    results = []\n",
    "    for question in questions:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5-nano\",\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"input_text\", \"text\": question}],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        results.append(response.output_text)\n",
    "\n",
    "    duration = time.time() - start\n",
    "    print(f\"‚è±Ô∏è  Synchronous: {duration:.2f} seconds\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cdd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ASYNCHRONOUS VERSION (Fast)\n",
    "async def async_multiple_calls():\n",
    "    \"\"\"Make 5 calls all at once using the Responses API.\"\"\"\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    async def ask(question):\n",
    "        response = await client.responses.create(\n",
    "            model=\"gpt-5-nano\",\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"input_text\", \"text\": question}],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return response.output_text\n",
    "\n",
    "    # Create all tasks\n",
    "    tasks = [ask(q) for q in questions]\n",
    "\n",
    "    # Run all in parallel\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    duration = time.time() - start\n",
    "    print(f\"‚ö° Asynchronous: {duration:.2f} seconds\")\n",
    "    return results\n",
    "\n",
    "print(\"Making 5 API calls...\\n\")\n",
    "sync_multiple_calls()\n",
    "await async_multiple_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971418f",
   "metadata": {},
   "source": [
    "## Practical Example: Batch Question Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_questions_batch(questions, model=\"gpt-5-mini\"):\n",
    "    \"\"\"Process multiple questions efficiently via the Responses API.\"\"\"\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    async def get_answer(question):\n",
    "        response = await client.responses.create(\n",
    "            model=model,\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"input_text\", \"text\": question}],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": response.output_text,\n",
    "            \"tokens\": response.usage.total_tokens,\n",
    "        }\n",
    "\n",
    "    # Process all questions in parallel\n",
    "    tasks = [get_answer(q) for q in questions]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "async def main():\n",
    "    questions = [\n",
    "        \"What is machine learning?\",\n",
    "        \"What is deep learning?\",\n",
    "        \"What is neural network?\",\n",
    "        \"What is GPT?\",\n",
    "        \"What is transformer architecture?\",\n",
    "    ]\n",
    "\n",
    "    print(f\"Processing {len(questions)} questions...\\n\")\n",
    "\n",
    "    results = await process_questions_batch(questions)\n",
    "\n",
    "    total_tokens = 0\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Q: {result['question']}\")\n",
    "        print(f\"   A: {result['answer'][:80]}...\")\n",
    "        print(f\"   Tokens: {result['tokens']}\")\n",
    "        total_tokens += result['tokens']\n",
    "        print()\n",
    "\n",
    "    print(f\"üìä Total tokens used: {total_tokens}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb6c2f",
   "metadata": {},
   "source": [
    "## Error Handling in Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "async def safe_api_call(client, question, model=\"gpt-5-mini\"):\n",
    "    \"\"\"API call with error handling using the Responses API.\"\"\"\n",
    "    try:\n",
    "        response = await client.responses.create(\n",
    "            model=model,\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"input_text\", \"text\": question}],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"answer\": response.output_text,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "async def main():\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    questions = [\n",
    "        \"What is Python?\",\n",
    "        \"What is AI?\",\n",
    "        \"What is blockchain?\",\n",
    "    ]\n",
    "\n",
    "    tasks = [safe_api_call(client, q) for q in questions]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        if result[\"success\"]:\n",
    "            print(f\"‚úÖ Question {i+1}: {result['answer'][:50]}...\")\n",
    "        else:\n",
    "            print(f\"‚ùå Question {i+1}: Error - {result['error']}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb0a9b",
   "metadata": {},
   "source": [
    "## When to Use Async vs Sync\n",
    "\n",
    "| Scenario | Use This | Why |\n",
    "|----------|----------|-----|\n",
    "| Single API call | Sync | Simpler code |\n",
    "| Multiple API calls | Async | Much faster |\n",
    "| Real-time chatbot | Sync | Sequential conversation |\n",
    "| Batch processing | Async | Process many at once |\n",
    "| Comparing models | Async | Get all responses together |\n",
    "| Simple script | Sync | Easier to understand |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0219ff",
   "metadata": {},
   "source": [
    "## üéØ Your Practice Tasks\n",
    "\n",
    "1. **Task 1:** Create an async function that asks 3 different models the same question and compares their response times.\n",
    "\n",
    "2. **Task 2:** Process this list of questions asynchronously:\n",
    "   ```python\n",
    "   questions = [\n",
    "       \"Define recursion\",\n",
    "       \"What is a REST API?\",\n",
    "       \"Explain Python decorators\",\n",
    "       \"What is async/await?\"\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **Task 3:** Time the difference - run 10 API calls synchronously, then asynchronously. Calculate the speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e253a",
   "metadata": {},
   "source": [
    "## ‚úÖ Key Takeaways\n",
    "\n",
    "‚úì Async = running multiple tasks in parallel\n",
    "\n",
    "‚úì Use `AsyncOpenAI` for async calls\n",
    "\n",
    "‚úì Use `await` for async operations\n",
    "\n",
    "‚úì Use `asyncio.gather()` to run multiple tasks together\n",
    "\n",
    "‚úì Async is 3-5x faster for multiple calls\n",
    "\n",
    "‚úì Perfect for batch processing and model comparison\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now know how to efficiently work with OpenAI models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cca29",
   "metadata": {},
   "source": [
    "## Quick Reference Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync (simple, one at a time)\n",
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"input_text\", \"text\": \"Explain AsyncIO in one sentence.\"}],\n",
    "        }\n",
    "    ],\n",
    " )\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58132905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async (fast, many at once)\n",
    "async def main():\n",
    "    client = AsyncOpenAI()\n",
    "    response = await client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": \"Explain AsyncIO in one sentence.\"}],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    print(response.output_text)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b2b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-level-up-in-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
