{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10277a5f",
   "metadata": {},
   "source": [
    "# Python for Learning AI Week 3: Building AI Assistants with Gemini API\n",
    "\n",
    "Welcome to Week 3 of our Python development series! This week, we're diving into the exciting world of Large Language Models (LLMs) by building our own AI assistant using Google's Gemini API. By the end of this session, you'll have created your own AI assistant and gained valuable insights into how modern AI systems work.\n",
    "\n",
    "**Superhero Sidekick Metaphor**: Throughout this notebook, we'll compare working with LLMs to having your own superhero AI sidekick:\n",
    "- The LLM is like your superhero sidekick with encyclopedic knowledge and special powers\n",
    "- Prompts are like mission briefings you give to your sidekick\n",
    "- API parameters are like special equipment you provide (e.g., \"be more creative\" or \"stay focused\")\n",
    "- The sidekick's responses are the intelligence and assistance they provide for your mission\n",
    "\n",
    "## What You'll Learn\n",
    "1. Understanding Large Language Models and Google's Gemini\n",
    "2. Setting up your API credentials securely\n",
    "3. Making requests to the Gemini API\n",
    "4. Customizing responses with different parameters\n",
    "5. Building a simple chatbot with persistent history\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge ([covered in Week 1](../week1/week1_python_basics.ipynb))\n",
    "- Understanding of APIs ([covered in Week 2](../week2/001_apis_and_networking.ipynb))\n",
    "- A Google account (to access the Gemini API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e42716",
   "metadata": {},
   "source": [
    "## 1. Setting Up Your Environment\n",
    "\n",
    "Before we can start using the Gemini API, we need to set up our environment properly. This includes adding our API key to the `.env` file and installing the necessary packages.\n",
    "\n",
    "### Installing Required Packages\n",
    "\n",
    "To set up your environment for using the Gemini API, you'll need to install the necessary packages. Follow these steps:\n",
    "\n",
    "1. Open a terminal window in VS Code (Terminal → New Terminal)\n",
    "2. Navigate to the project root directory (where the `pyproject.toml` file is located)\n",
    "3. Run the following command to install all project dependencies using UV:\n",
    "   ```bash\n",
    "   uv pip install -e .\n",
    "   ```\n",
    "   This command installs the project in \"editable\" mode, allowing you to modify the code without reinstalling.\n",
    "\n",
    "4. Install the Google Genai library (if it's not already in the project dependencies):\n",
    "   ```bash\n",
    "   uv pip install google-genai\n",
    "   ```\n",
    "   \n",
    "   The `google-genai` package is Google's latest official Python client library for the Gemini API. It provides:\n",
    "   - Simple methods to interact with Google's Generative AI models\n",
    "   - Tools for sending prompts and receiving responses\n",
    "   - Support for chat-based interactions with conversation history\n",
    "   - Structured data handling and formatting options with Pydantic integration\n",
    "   - Authentication and API key management\n",
    "   - Modern API design with improved developer experience\n",
    "\n",
    "5. You can also check for the latest versions of packages and update them:\n",
    "   ```bash\n",
    "   uv pip install --upgrade google-genai python-dotenv\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a4023",
   "metadata": {},
   "source": [
    "### Configuring Your Gemini API Key\n",
    "\n",
    "To use the Gemini API, you need an API key. Follow these steps:\n",
    "\n",
    "1. Visit [Google AI Studio](https://aistudio.google.com/) and sign in with your Google account. \n",
    "   *Note: Using your personal Gmail ID is perfectly fine for this exercise.*\n",
    "2. Click on \"Get API key\" in the side nav bar.\n",
    "3. Click on \"Create API key\" on top right corner.\n",
    "4. Provide a suitable name for the API key.\n",
    "5. Create a new project in which the API key will be created.\n",
    "6. Create your API key.\n",
    "7. Click on the created API key you created from the table, copy the API key (Starts with `AI`).\n",
    "\n",
    "Now, add your API key to the existing `.env` file in the project root directory by adding these lines:\n",
    "```\n",
    "GEMINI_API_KEY=your_api_key_here\n",
    "GEMINI_MODEL=gemini-2.5-flash\n",
    "```\n",
    "\n",
    "You can change the model to any of the available models like:\n",
    "- gemini-2.5-flash\n",
    "- gemini-2.5-flash-lite\n",
    "- gemini-2.5-pro\n",
    "\n",
    "⚠️ **Important:** Never share your API key with others or commit it to public repositories!\n",
    "\n",
    "Let's check if we can load the API key from the `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ecfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file and initialize the Google Genai library\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "# Set default model as an environment variable\n",
    "# You can add this to your .env file too for persistence\n",
    "GEMINI_MODEL = os.getenv('GEMINI_MODEL')\n",
    "\n",
    "print(f\"Using Gemini model: {GEMINI_MODEL}\")\n",
    "\n",
    "# Check if the API key exists and is properly formatted\n",
    "if api_key and api_key.startswith('AI'):\n",
    "    print(\"✅ API key loaded successfully!\")\n",
    "    # Show a masked version of the key for verification (first 4 chars and last 4 chars)\n",
    "    masked_key = api_key[:4] + '*' * (len(api_key) - 8) + api_key[-4:]\n",
    "    print(f\"API Key: {masked_key}\")\n",
    "    \n",
    "    # Configure the client with our key\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    print(\"List of models that support generateContent:\\n\")\n",
    "    for m in client.models.list():\n",
    "        for action in m.supported_actions:\n",
    "            if action == \"generateContent\":\n",
    "                print(m.name)\n",
    "\n",
    "    print(\"List of models that support embedContent:\\n\")\n",
    "    for m in client.models.list():\n",
    "        for action in m.supported_actions:\n",
    "            if action == \"embedContent\":\n",
    "                print(m.name)\n",
    "else:\n",
    "    print(\"❌ API key not found or invalid.\")\n",
    "    print(\"Please add GEMINI_API_KEY=your_api_key_here to your .env file in the project root directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2640b7ba",
   "metadata": {},
   "source": [
    "## 2. Making Simple Requests to Gemini\n",
    "\n",
    "Now that we've set up our environment and connected to the API, let's start by making a simple request to the Gemini model. We'll create a function that allows us to send a prompt and receive a response.\n",
    "\n",
    "### Understanding the API Flow\n",
    "\n",
    "In this notebook, we're interacting with Google's hosted Large Language Models (LLMs) through their Gemini API. Let's understand the basic flow:\n",
    "\n",
    "```\n",
    "┌───────────────────┐      HTTP Request       ┌─────────────────┐\n",
    "│                   │                         │                 │\n",
    "│    Our Code       │                         │                 │\n",
    "│    (Python)       │                         │  Google's LLM   │\n",
    "│                   │                         │  (Cloud-hosted) │\n",
    "│ ┌─────────────┐   │                         │                 │\n",
    "│ │ genai.Client│───────────────────────────> │                 │\n",
    "│ └─────────────┘   │   API Key + Prompt      │                 │\n",
    "│                   │ <─────────────────────  │                 │\n",
    "│                   │    Generated Content    │                 │\n",
    "└───────────────────┘                         └─────────────────┘\n",
    "```\n",
    "\n",
    "**About the Gemini API with the google-genai Client**:\n",
    "- **API Type**: Modern Python client for Google's Gemini models\n",
    "- **Primary Client Methods**: \n",
    "  - `client.models.generate_content()` - For one-off content generation requests\n",
    "  - `client.chats.create()` - Creates conversation sessions\n",
    "  - `chat.send_message()` - For multi-turn conversations\n",
    "- **Authentication**: Uses API key-based authentication via the Client object\n",
    "- **Request Format**: Structured Python objects converted to API requests\n",
    "- **Response Format**: Rich response objects with text, metadata, and usage statistics\n",
    "\n",
    "**What's happening:**\n",
    "1. We initialize a `genai.Client()` with our API key\n",
    "2. The client handles connection details, authentication, and request formatting\n",
    "3. Google's servers process our prompts using their trained LLMs (Gemini models)\n",
    "4. The client parses the responses into structured objects with helpful methods and properties\n",
    "5. We access the results through these objects (e.g., `response.text`, `response.usage_metadata`)\n",
    "\n",
    "In this notebook, we'll be using both the client's `models.generate_content()` method for single requests and the chat functionality (`client.chats.create()` and `chat.send_message()`) for conversation-based interactions to demonstrate different ways of working with the modern Gemini API.\n",
    "\n",
    "**Superhero Sidekick Metaphor**: This is like giving your superhero sidekick a simple reconnaissance mission—perhaps asking them to scout ahead and report what they see. We're starting with something simple to test their capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5149c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for basic interaction with Gemini\n",
    "def ask_gemini(prompt, model_name=None):\n",
    "    \"\"\"\n",
    "    Send a prompt to the Gemini model and get a response.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the model\n",
    "        model_name (str, optional): The name of the Gemini model to use, defaults to environment variable\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    # Use the model from environment variable if not specified\n",
    "    if model_name is None:\n",
    "        model_name = GEMINI_MODEL\n",
    "        \n",
    "    print(f\"Using model: {model_name}\")\n",
    "    \n",
    "    # Initialize client\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Generate a response\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a simple prompt\n",
    "prompt = \"Who are some super heroes who can code in Python?\"\n",
    "response = ask_gemini(prompt)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0771dbf",
   "metadata": {},
   "source": [
    "## 3. Customizing Model Behavior with Parameters\n",
    "\n",
    "One powerful aspect of using LLMs like Gemini is that you can customize various parameters to control the behavior of the model. Let's explore the most important ones:\n",
    "\n",
    "- **Temperature**: Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
    "- **Top-k**: Limits token selection to the k most likely next tokens\n",
    "- **Top-p**: Limits token selection to a subset with a cumulative probability of p\n",
    "\n",
    "**Note**: Depending on the specific model you're using (e.g., gemini-2.5-flash vs. gemini-2.5-pro), there might be variations in how these parameters affect the output. Some models may have additional parameters or different optimal ranges. Always refer to the documentation for your chosen model for the most accurate information.\n",
    "\n",
    "**Superhero Sidekick Metaphor**: These parameters are like configuring your sidekick's approach to the mission:\n",
    "- Temperature is like adjusting how creative or by-the-book your sidekick should be\n",
    "- Top-k is like limiting their toolkit to only the most reliable gadgets\n",
    "- Top-p is like telling them to only use strategies they're confident in\n",
    "\n",
    "**Want to learn more about these parameters?**\n",
    "- [Google AI: Gemini API Parameter Guide](https://ai.google.dev/docs/concepts#model_parameters)\n",
    "- [Understanding Temperature in LLMs](https://www.promptingguide.ai/introduction/settings.en#temperature)\n",
    "- [OpenAI API Parameters Guide](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature) (similar concepts apply across LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced function with customizable parameters\n",
    "def ask_gemini_custom(prompt, temperature=0.7, top_k=40, top_p=0.95, model_name=None):\n",
    "    \"\"\"\n",
    "    Send a prompt to the Gemini model with customizable parameters.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the model\n",
    "        temperature (float): Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
    "        top_k (int): Limits token selection to the k most likely next tokens\n",
    "        top_p (float): Limits token selection to a subset with cumulative probability of p\n",
    "        model_name (str, optional): The name of the Gemini model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    # Use the model from environment variable if not specified\n",
    "    if model_name is None:\n",
    "        model_name = GEMINI_MODEL\n",
    "        \n",
    "    # Configure generation parameters\n",
    "    generation_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "    }\n",
    "    \n",
    "    # Initialize client\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Generate a response with the specified configuration\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=prompt,\n",
    "        config=generation_config\n",
    "    )\n",
    "    \n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the same prompt with different temperatures\n",
    "# Rerun the below cells, to check the consistency of responses\n",
    "\n",
    "prompt = \"Write a very short poem about super heroes using respective movie references.\"\n",
    "\n",
    "print(\"Temperature = 0.2 (More focused, less creative)\")\n",
    "print(\"-\" * 50)\n",
    "response_cold = ask_gemini_custom(prompt, temperature=0.2)\n",
    "print(response_cold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea6a8d",
   "metadata": {},
   "source": [
    "### Experiment Time: Play with Parameters!\n",
    "\n",
    "Now it's your turn to experiment! Try running the following cells multiple times to see how different temperature values affect the responses. You can also modify the code to:\n",
    "\n",
    "1. **Try different temperature values** (between 0.0 and 1.0)\n",
    "2. **Experiment with top_k** (try values like 10, 20, 40, 100) \n",
    "3. **Adjust top_p** (try values between 0.5 and 1.0)\n",
    "4. **Ask different questions** to see how parameters affect various types of requests\n",
    "\n",
    "The best way to understand these parameters is to experiment with them yourself. Notice how lower temperatures produce more consistent responses, while higher temperatures create more varied and creative outputs.\n",
    "\n",
    "**Challenge**: After running the cells below, try creating your own cell that uses very different parameter combinations to see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Temperature = 1.0 (More creative, more diverse)\")\n",
    "print(\"-\" * 50)\n",
    "response_hot = ask_gemini_custom(prompt, temperature=1.0)\n",
    "print(response_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56bebb8",
   "metadata": {},
   "source": [
    "### Understanding Tokens and Monitoring in LLM Platforms\n",
    "\n",
    "When working with LLMs like Gemini, it's important to understand how tokens affect your usage and costs. Tokens are the basic units that LLMs process - they can be parts of words, words, or punctuation.\n",
    "\n",
    "**Monitoring in Google AI Studio**\n",
    "\n",
    "For detailed analysis of your Gemini API usage:\n",
    "\n",
    "1. Go to [Google AI Studio](https://aistudio.google.com/)\n",
    "2. Click on \"API Keys\" in the sidebar\n",
    "3. Select the API key you're using\n",
    "4. View the \"Usage\" tab to see:\n",
    "   - Token consumption over time\n",
    "   - API call frequency\n",
    "   - Model usage distribution\n",
    "   - Cost estimates (if applicable)\n",
    "\n",
    "**Alternative: OpenAI Tokenizer Tools**\n",
    "\n",
    "If you're working with multiple LLM providers or want to analyze token usage outside of Google AI Studio:\n",
    "\n",
    "1. Use OpenAI's [Tokenizer Tool](https://platform.openai.com/tokenizer) - a free web interface that shows exactly how text is split into tokens\n",
    "\n",
    "This monitoring is essential for managing costs and optimizing your prompts, especially in production applications where efficiency matters.\n",
    "\n",
    "Remember that different LLMs use slightly different tokenization methods, but the token counts are generally similar enough for estimation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a25a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating token counting with google-genai\n",
    "# This shows how to count tokens before sending a request and get usage metadata after\n",
    "\n",
    "# Initialize client \n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "# Sample prompt\n",
    "prompt = \"Write a short paragraph about how Iron Man's technology works.\"\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "\n",
    "# Count tokens in the prompt before sending\n",
    "token_count = client.models.count_tokens(\n",
    "    model=GEMINI_MODEL, \n",
    "    contents=prompt\n",
    ")\n",
    "print(\"\\nToken count before generation:\")\n",
    "print(f\"Prompt tokens: {token_count.total_tokens}\")\n",
    "\n",
    "# Generate content with the prompt\n",
    "response = client.models.generate_content(\n",
    "    model=GEMINI_MODEL,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "print(\"\\nResponse:\")\n",
    "print(response.text[:200] + \"...\" if len(response.text) > 200 else response.text)\n",
    "\n",
    "# Show detailed token usage statistics\n",
    "print(\"\\nDetailed token usage statistics:\")\n",
    "print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
    "print(f\"Response tokens: {response.usage_metadata.candidates_token_count}\")\n",
    "print(f\"Total tokens: {response.usage_metadata.total_token_count}\")\n",
    "print(f\"Token rate: {response.usage_metadata.prompt_token_count / len(prompt):.2f} tokens per character\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af151938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Demonstrate with a more complex prompt\n",
    "complex_prompt = \"\"\"\n",
    "Analyze the following technologies and compare their efficiency:\n",
    "1. Iron Man's Arc Reactor\n",
    "2. Black Panther's Vibranium suit\n",
    "3. Batman's gadgets\n",
    "4. Wonder Woman's equipment\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\\nComplex prompt token analysis:\")\n",
    "complex_token_count = client.models.count_tokens(\n",
    "    model=GEMINI_MODEL, \n",
    "    contents=complex_prompt\n",
    ")\n",
    "print(f\"Complex prompt tokens: {complex_token_count.total_tokens}\")\n",
    "print(f\"Character count: {len(complex_prompt)}\")\n",
    "print(f\"Token to character ratio: {complex_token_count.total_tokens / len(complex_prompt):.2f}\")\n",
    "\n",
    "print(\"\\nUnderstanding token counts helps optimize your API usage and costs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adacb0f",
   "metadata": {},
   "source": [
    "## 4. Building a Simple Chatbot with History\n",
    "\n",
    "So far, we've been making one-off requests to the Gemini model. However, for a true chatbot experience, we need to maintain conversation history to provide context for follow-up questions. The Gemini API makes this easy with its chat functionality.\n",
    "\n",
    "**Superhero Sidekick Metaphor**: This is like having an ongoing mission with your sidekick, where they remember all the previous intel and context, allowing for a more coordinated operation rather than treating each interaction as a brand new mission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that prepares a chat session for a superhero-themed conversation\n",
    "def setup_hero_chat(model_name=None):\n",
    "    \"\"\"\n",
    "    Set up a chat session for a superhero-themed conversation.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str, optional): The name of the Gemini model to use\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The chat session object and the client object\n",
    "    \"\"\"\n",
    "    # Use the model from environment variable if not specified\n",
    "    if model_name is None:\n",
    "        model_name = GEMINI_MODEL\n",
    "        \n",
    "    print(f\"Setting up superhero chat with model: {model_name}\")\n",
    "    \n",
    "    # Initialize client\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Create a chat session\n",
    "    chat = client.chats.create(\n",
    "        model=model_name,\n",
    "        history=[]  # Start with an empty conversation\n",
    "    )\n",
    "    \n",
    "    print(\"Superhero chat session initialized and ready for your questions!\")\n",
    "    print(\"This chat maintains context between questions, like a true sidekick.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return chat, client\n",
    "\n",
    "# Initialize our superhero chat session\n",
    "hero_chat, hero_client = setup_hero_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Ask about scientist superheroes\n",
    "question1 = \"Who are the most popular superheroes that are also scientists?\"\n",
    "print(f\"Question: {question1}\\n\")\n",
    "\n",
    "# Send the message and get the response\n",
    "try:\n",
    "    response1 = hero_chat.send_message(question1)\n",
    "    print(f\"Gemini: {response1.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8525bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Ask about Iron Man's scientific background\n",
    "question2 = \"Can you describe the scientific background of Tony Stark/Iron Man?\"\n",
    "print(f\"Question: {question2}\\n\")\n",
    "\n",
    "# Send the message and get the response\n",
    "try:\n",
    "    response2 = hero_chat.send_message(question2)\n",
    "    print(f\"Gemini: {response2.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871981aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Ask about Black Panther's vibranium technology\n",
    "question3 = \"What scientific principles does Black Panther's vibranium technology use?\"\n",
    "print(f\"Question: {question3}\\n\")\n",
    "\n",
    "# Send the message and get the response\n",
    "try:\n",
    "    response3 = hero_chat.send_message(question3)\n",
    "    print(f\"Gemini: {response3.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa106fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus question: Ask for a comparison that references previous answers\n",
    "bonus_question = \"Compare the scientific approaches of previous hero and Black Panther. Which is more advanced?\"\n",
    "print(f\"Question: {bonus_question}\\n\")\n",
    "\n",
    "# Send the message and get the response\n",
    "try:\n",
    "    bonus_response = hero_chat.send_message(bonus_question)\n",
    "    print(f\"Gemini: {bonus_response.text}\")\n",
    "    print(\"\\nNotice how the model remembers the context from previous questions and can make comparisons!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d605c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question!\n",
    "# Replace this with any superhero-related question you want to ask\n",
    "your_question = \"What would happen if Spider-Man and Batman teamed up?\"\n",
    "\n",
    "print(f\"Your Question: {your_question}\\n\")\n",
    "\n",
    "# Send the message and get the response\n",
    "try:\n",
    "    your_response = hero_chat.send_message(your_question)\n",
    "    print(f\"Gemini: {your_response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n",
    "print(\"\\nTry modifying this cell with different questions to see how the conversation continues!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058bc8d",
   "metadata": {},
   "source": [
    "## 5. Implementing Persistent Chat History\n",
    "\n",
    "Now let's create a class that can save chat history to a file for future reference.\n",
    "### Benefits of Persisting Chat History\n",
    "\n",
    "Saving chat history to disk provides several advantages for AI applications:\n",
    "\n",
    "1. **Continuity Between Sessions**: Users can pick up conversations where they left off, even after closing the application.\n",
    "\n",
    "2. **Analytics and Improvement**: \n",
    "   - Track common questions and improve responses\n",
    "   - Identify patterns in user interactions\n",
    "   - Use historical data to fine-tune future AI models\n",
    "\n",
    "3. **User Experience**: \n",
    "   - Maintain context for longer, more meaningful conversations\n",
    "   - Provide personalized responses based on past interactions\n",
    "   - Create a \"memory\" that builds relationships with users\n",
    "\n",
    "4. **Debugging and Quality Control**:\n",
    "   - Review past interactions to identify where the AI might have provided incorrect information\n",
    "   - Understand how conversations evolve and where they might go off-track\n",
    "\n",
    "5. **Application Integration**: The saved JSON data can be easily integrated with databases, dashboards, or other tools for further processing.\n",
    "\n",
    "In our superhero sidekick metaphor, this is like your sidekick keeping a detailed mission log that you can review together later, helping both of you improve your teamwork and tactics for future missions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8958cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for persistent chat using google-genai\n",
    "import datetime\n",
    "import json\n",
    "class PersistentChat:\n",
    "    def __init__(self, model_name=None, history_file='chat_history.json'):\n",
    "        \"\"\"\n",
    "        Initialize a chat with persistent history.\n",
    "        \n",
    "        This class enables conversations with Gemini that are saved to disk, allowing the\n",
    "        chat history to persist between sessions and be analyzed later. This is especially\n",
    "        useful for applications that need to maintain context over time, track user interactions,\n",
    "        or analyze conversation patterns for further improvements.\n",
    "        \"\"\"\n",
    "        # Use the model from environment variable if not specified\n",
    "        if model_name is None:\n",
    "            model_name = GEMINI_MODEL\n",
    "            \n",
    "        # Initialize the client\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "        self.history_file = os.path.join('../', history_file)\n",
    "        self.chat_history = []\n",
    "        \n",
    "        # Create a chat session with the client\n",
    "        self.chat = self.client.chats.create(model=model_name, history=[])\n",
    "    \n",
    "    def send_message(self, message):\n",
    "        \"\"\"Send a message and save the response to history.\"\"\"\n",
    "        response = self.chat.send_message(message)\n",
    "        \n",
    "        # Add to history\n",
    "        self.chat_history.append({\"role\": \"user\", \"message\": message})\n",
    "        self.chat_history.append({\"role\": \"assistant\", \"message\": response.text})\n",
    "        \n",
    "        # Save history\n",
    "        self.save_history()\n",
    "        \n",
    "        return response.text\n",
    "    \n",
    "    def save_history(self):\n",
    "        \"\"\"Save chat history to file.\"\"\"\n",
    "        # Create a timestamped record\n",
    "        history_record = {\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"conversations\": self.chat_history\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(self.history_file, 'w') as f:\n",
    "                json.dump(history_record, f, indent=2)\n",
    "            print(f\"Chat history saved to {self.history_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving chat history: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc217589",
   "metadata": {},
   "source": [
    "### Continuing a Persistent Chat Session\n",
    "\n",
    "One of the key benefits of the `PersistentChat` class is that you can easily save your conversation and resume it later. This allows users to maintain context across multiple sessions, creating a more coherent and personalized experience.\n",
    "\n",
    "Here's how you can continue a conversation from a previous session:\n",
    "\n",
    "1. **Create a new instance** of `PersistentChat` with the same `history_file` parameter\n",
    "2. **Send new messages** to continue the conversation where you left off\n",
    "3. **Access the full conversation history** from the JSON file whenever needed\n",
    "\n",
    "This enables several powerful use cases:\n",
    "- Building AI assistants that remember user preferences\n",
    "- Creating long-running conversations that span days or weeks\n",
    "- Analyzing conversation patterns over time\n",
    "- Sharing conversation contexts between different applications\n",
    "\n",
    "Example of continuing a conversation:\n",
    "```python\n",
    "# Start a conversation\n",
    "chat = PersistentChat(history_file='my_hero_chat.json')\n",
    "chat.send_message(\"Tell me about Wonder Woman\")\n",
    "\n",
    "# Later, even after restarting your application\n",
    "continued_chat = PersistentChat(history_file='my_hero_chat.json') \n",
    "continued_chat.send_message(\"What are her greatest powers?\")  # The AI remembers the context\n",
    "```\n",
    "\n",
    "The history is stored in a JSON file that includes timestamps and a complete record of the conversation, making it easy to integrate with other systems or analyze later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unlike the previous chat examples, PersistentChat saves all interactions to a file,\n",
    "# allowing you to review past conversations, analyze responses, or even train future models.\n",
    "print(\"\\nExample of persistent chat:\")\n",
    "persistent_chat = PersistentChat(history_file='./superhero_chat_demo.json')\n",
    "response1 = persistent_chat.send_message(\"Who would win in a fight between Batman and Iron Man, and why?\")\n",
    "print(f\"Response: {response1[:150]}...\")  # Show just the beginning for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4cfcb3",
   "metadata": {},
   "source": [
    "## 6. Conclusion: Your AI Superhero Sidekick Journey\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. Set up your environment with the Gemini API\n",
    "2. Made basic requests to generate text\n",
    "3. Customized model behavior with parameters\n",
    "4. Built a simple chatbot with conversation history\n",
    "5. Worked with structured data responses\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different parameters to see how they affect the responses\n",
    "- Try creating a specialized assistant for a specific domain (e.g., a superhero encyclopedia or a comic book advisor)\n",
    "- Explore more advanced prompt engineering techniques\n",
    "- Try integrating Gemini into a web application or other projects\n",
    "\n",
    "**Resources:**\n",
    "- [Google AI Studio](https://aistudio.google.com/)\n",
    "- [Gemini API Documentation](https://ai.google.dev/docs)\n",
    "- [Prompt Engineering Guide](https://ai.google.dev/docs/prompt_best_practices)\n",
    "\n",
    "Remember that LLM capabilities are constantly evolving, so keep exploring and learning as these technologies advance!\n",
    "\n",
    "Happy coding and AI creating!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-level-up-in-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
